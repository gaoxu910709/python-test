# -*- coding: utf-8 -*-
""" é­…å½±å›¾åº“çˆ¬è™« åŒå¹³å°é€šç”¨ç‰ˆï¼ˆæ‰‹æœºTermux+Windowsç”µè„‘ï¼‰| é¡ºåºä¸‹è½½ | 40KBè¿‡æ»¤ | é­”æ³•æ•°å­—æ ¡éªŒ """
import requests
from bs4 import BeautifulSoup
import concurrent.futures
from PIL import Image
import io
import os
import re
import sys
import time
import random
import argparse
import logging
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# -------- æ—¥å¿—è®¾ç½®ï¼ˆåŒå¹³å°ï¼šç»ˆç«¯+æ–‡ä»¶ï¼Œä¸­æ–‡å…¼å®¹ï¼‰ --------
log_file = "é­…å½±å›¾åº“_crawler.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# ç¦ç”¨ç¬¬ä¸‰æ–¹åº“æ—¥å¿—
for name in logging.root.manager.loggerDict:
    if name not in ['__main__', 'crawler']:
        logging.getLogger(name).setLevel(logging.CRITICAL)

# -------- è·¨å¹³å°æ ¸å¿ƒé…ç½® --------
IS_MOBILE = os.path.exists("/sdcard/Download")  # è‡ªåŠ¨è¯†åˆ«æ‰‹æœº/Windows
MIN_IMAGE_SIZE = 40 * 1024  # 40KBæ–‡ä»¶è¿‡æ»¤
# è·¨å¹³å°é»˜è®¤ä¿å­˜è·¯å¾„
DEFAULT_SAVE_DIR_MOBILE = "/sdcard/Download/é­…å½±å›¾åº“"
DEFAULT_SAVE_DIR_WIN = r"C:\çˆ¬å–ç»“æœ\é­…å½±å›¾åº“"

class GalleryCrawler:
    def __init__(self, save_path, verify=False):
        self.save_path = save_path
        self.verify = verify
        self.session = self._create_session()
        # åˆå§‹åŒ–åˆ—è¡¨
        self.completed_list = []
        self.failed_list = []
        self.processed_album_urls = set()  # è®°å½•å·²å¤„ç†ä¸“è¾‘ï¼Œé¿å…é‡å¤
        # åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆè·¨å¹³å°å…¼å®¹ï¼‰
        os.makedirs(self.save_path, exist_ok=True)
        logger.info(f"ğŸ“‚ åˆ›å»º/æ£€æµ‹ä¿å­˜ç›®å½•: {self.save_path}")

    def _create_session(self):
        """åˆ›å»ºå¸¦è¿æ¥æ± å’Œé‡è¯•æœºåˆ¶çš„sessionï¼Œè·¨å¹³å°å…¼å®¹"""
        session = requests.Session()
        retry = Retry(
            total=5,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )
        adapter = HTTPAdapter(max_retries=retry, pool_connections=100, pool_maxsize=100)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        # è®¾ç½®headersï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨+é˜²ç›—é“¾
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://xxtu.org/',
            'Accept-Language': 'zh-CN,zh;q=0.9'
        })
        return session

    def _sanitize_filename(self, filename):
        """è·¨å¹³å°æ–‡ä»¶åæ¸…æ´—ï¼Œè¿‡æ»¤æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦"""
        return re.sub(r'[\\/:*?"<>|+@#$%^&*(){}[]Â·~!ï¿¥]', ' ', filename)

    def get_single_page_albums(self, page):
        """è·å–å•é¡µç›¸å†Œé“¾æ¥å’Œåç§°ï¼Œç”¨äºé¡ºåºçˆ¬å–"""
        base_url = "https://xxtu.org/"
        try:
            # æ„å»ºåˆ†é¡µURL
            current_url = base_url if page == 1 else f"{base_url}?paged={page}"
            logger.info(f"\nğŸ“„ æ­£åœ¨è·å–ç¬¬ {page} é¡µç›¸å†Œï¼ŒURL: {current_url}")
            # éšæœºå»¶è¿Ÿ4-8ç§’ï¼Œé˜²åçˆ¬
            delay = random.uniform(4, 8)
            logger.info(f"â±ï¸  éšæœºå»¶è¿Ÿ {delay:.1f} ç§’...")
            time.sleep(delay)

            # è·å–é¡µé¢å†…å®¹
            start_time = time.time()
            response = self.session.get(current_url, timeout=30)
            response.raise_for_status()
            end_time = time.time()

            # è®¡ç®—ä¸‹è½½ä¿¡æ¯
            content_length = len(response.content)
            elapsed_time = end_time - start_time
            if elapsed_time > 0:
                speed = content_length / elapsed_time / 1024  # KB/s
                logger.info(f"ğŸ“¥ é¡µé¢ä¸‹è½½å®Œæˆï¼Œå¤§å°: {content_length/1024:.1f} KBï¼Œè€—æ—¶: {elapsed_time:.2f} ç§’ï¼Œé€Ÿåº¦: {speed:.1f} KB/s")

            # è§£æé¡µé¢
            logger.info(f"ğŸ” æ­£åœ¨è§£æé¡µé¢...")
            soup = BeautifulSoup(response.text, 'html.parser')
            # æŸ¥æ‰¾æ‰€æœ‰ç›¸å†Œé¡¹
            album_items = soup.find_all('article') or soup.find_all('div', class_='post')
            if not album_items:
                logger.info(f"ç¬¬ {page} é¡µæœªæ‰¾åˆ°ç›¸å†Œé¡¹")
                return [], None

            logger.info(f"âœ… ç¬¬ {page} é¡µæ‰¾åˆ° {len(album_items)} ä¸ªç›¸å†Œé¡¹")
            # æå–ç›¸å†Œä¿¡æ¯
            albums = []
            for item in album_items:
                a_tag = item.find('a')
                if a_tag and 'href' in a_tag.attrs:
                    album_url = a_tag['href']
                    if album_url in self.processed_album_urls:
                        continue
                    # æŸ¥æ‰¾ç›¸å†Œåç§°ï¼Œå¤šæ ‡ç­¾å…¼å®¹
                    title_tag = item.find('h2', class_='entry-title') or item.find('h1', class_='entry-title') or item.find('h3', class_='entry-title')
                    if title_tag:
                        album_name = title_tag.text.strip()
                        sanitized_name = self._sanitize_filename(album_name)
                        albums.append((sanitized_name, album_name, album_url))
                        self.processed_album_urls.add(album_url)
                        logger.info(f"ğŸ‰ æ£€ç´¢åˆ°æ–°ç›¸å†Œ: {album_name}")

            # æ£€æŸ¥ä¸‹ä¸€é¡µ
            next_page = page + 1 if len(albums) > 0 else None
            return albums, next_page

        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 404:
                logger.info(f"ç¬¬ {page} é¡µè¿”å›404ï¼Œå·²åˆ°è¾¾æœ€åä¸€é¡µ")
                return [], None
            else:
                logger.error(f"HTTPè¯·æ±‚å¤±è´¥: {e}ï¼Œè¯·æŒ‰ä»»æ„é”®é‡è¯•...")
                input()
                return [], page
        except requests.exceptions.RequestException as e:
            logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}ï¼Œè¯·æŒ‰ä»»æ„é”®é‡è¯•...")
            input()
            return [], page
        except Exception as e:
            logger.error(f"è·å–ç¬¬ {page} é¡µç›¸å†Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return [], None

    def validate_image(self, image_path):
        """éªŒè¯å›¾ç‰‡æ˜¯å¦æŸåï¼Œå…ˆæ£€æŸ¥å¤§å°å†æ ¡éªŒå®Œæ•´æ€§"""
        # å…ˆè¿‡æ»¤å°äº40KBçš„æ–‡ä»¶
        if os.path.getsize(image_path) < MIN_IMAGE_SIZE:
            logger.error(f"å›¾ç‰‡ {image_path} å°äº40KBï¼Œåˆ¤å®šä¸ºæ— æ•ˆ")
            return False
        # éªŒè¯å›¾ç‰‡å®Œæ•´æ€§
        try:
            with Image.open(image_path) as img:
                img.verify()
            return True
        except Exception as e:
            logger.error(f"å›¾ç‰‡ {image_path} æŸå: {e}")
            return False

    def download_image(self, image_url, save_path):
        """ä¸‹è½½å•å¼ å›¾ç‰‡ï¼Œ40KBè¿‡æ»¤+é­”æ³•æ•°å­—æ ¡éªŒ+åŸå­åŒ–å†™å…¥"""
        retry_count = 0
        max_retries = 5
        img_name = os.path.basename(save_path)
        # è·¨å¹³å°æ¸…æ´—å›¾ç‰‡å
        img_name = self._sanitize_filename(img_name)
        save_path = os.path.join(os.path.dirname(save_path), img_name)

        while retry_count < max_retries:
            try:
                # éšæœºå»¶è¿Ÿ4-8ç§’
                delay = random.uniform(4, 8)
                logger.info(f"[{img_name}] â±ï¸  éšæœºå»¶è¿Ÿ {delay:.1f} ç§’...")
                time.sleep(delay)

                # å‘é€è¯·æ±‚
                logger.info(f"[{img_name}] ğŸ”— æ­£åœ¨è¿æ¥: {image_url}")
                response = self.session.get(image_url, timeout=60, stream=True)
                response.raise_for_status()

                # éªŒè¯å“åº”å†…å®¹æ˜¯å¦ä¸ºå›¾ç‰‡
                content_type = response.headers.get('Content-Type', '')
                if not content_type.startswith('image/'):
                    logger.error(f"[{img_name}] âŒ è¿”å›éå›¾ç‰‡å†…å®¹: {content_type}")
                    retry_count += 1
                    continue

                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                if os.path.exists(save_path):
                    if self.validate_image(save_path):
                        logger.info(f"[{img_name}] âœ… å·²å­˜åœ¨ä¸”å®Œæ•´ï¼Œè·³è¿‡")
                        return True
                    else:
                        logger.info(f"[{img_name}] ğŸ”„ å·²å­˜åœ¨ä½†æŸåï¼Œé‡æ–°ä¸‹è½½")

                # åŸå­åŒ–å†™å…¥æ–‡ä»¶ï¼ˆè·¨å¹³å°ç›®å½•å…¼å®¹ï¼‰
                temp_path = save_path + '.tmp'
                downloaded_size = 0
                start_time = time.time()

                with open(temp_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            downloaded_size += len(chunk)
                            # ç®€å•è¿›åº¦æ˜¾ç¤º
                            elapsed_time = time.time() - start_time
                            if elapsed_time > 0.5:
                                speed = downloaded_size / elapsed_time / 1024
                                logger.info(f"[{img_name}] ğŸ“Š ä¸‹è½½ä¸­: {downloaded_size/1024:.1f} KB | é€Ÿåº¦: {speed:.1f} KB/s", end='\r')

                print()  # æ¢è¡Œç»“æŸè¿›åº¦æ¡
                # è¿‡æ»¤å°äº40KBçš„æ–‡ä»¶
                if downloaded_size < MIN_IMAGE_SIZE:
                    logger.error(f"[{img_name}] âŒ æ–‡ä»¶è¿‡å°({downloaded_size/1024:.1f} KB < 40KB)ï¼Œä¸¢å¼ƒ")
                    os.remove(temp_path)
                    retry_count += 1
                    continue

                # é­”æ³•æ•°å­—éªŒè¯ï¼ˆå›¾ç‰‡æ ¼å¼æ ¡éªŒï¼‰
                logger.info(f"[{img_name}] ğŸ” éªŒè¯å›¾ç‰‡å®Œæ•´æ€§...")
                with open(temp_path, 'rb') as f:
                    magic_number = f.read(8)
                valid_magic_numbers = {b'\xFF\xD8\xFF', b'\x89\x50\x4E\x47', b'\x47\x49\x46\x38', b'\x42\x4D'}
                is_valid = any(magic_number.startswith(m) for m in valid_magic_numbers)
                if not is_valid:
                    logger.error(f"[{img_name}] âŒ é­”æ³•æ•°å­—æ— æ•ˆï¼Œä¸¢å¼ƒ")
                    os.remove(temp_path)
                    retry_count += 1
                    continue

                # æœ€ç»ˆéªŒè¯å¹¶é‡å‘½å
                if self.validate_image(temp_path):
                    os.rename(temp_path, save_path)
                    logger.info(f"[{img_name}] âœ… ä¸‹è½½å®Œæˆï¼Œä¿å­˜è‡³: {save_path}")
                    return True
                else:
                    logger.error(f"[{img_name}] âŒ ä¸‹è½½åæŸåï¼Œé‡è¯•({retry_count+1}/{max_retries})")
                    os.remove(temp_path)
                    retry_count += 1
            except requests.exceptions.RequestException as e:
                retry_count += 1
                logger.error(f"[{img_name}] âŒ ç½‘ç»œå¤±è´¥: {e}ï¼Œé‡è¯•({retry_count}/{max_retries})")
                time.sleep(random.uniform(4, 8))
            except Exception as e:
                retry_count += 1
                logger.error(f"[{img_name}] âŒ ä¸‹è½½å¤±è´¥: {e}ï¼Œé‡è¯•({retry_count}/{max_retries})")
                time.sleep(random.uniform(4, 8))

        logger.error(f"[{img_name}] âŒ 5æ¬¡é‡è¯•å‡å¤±è´¥ï¼Œæ”¾å¼ƒä¸‹è½½")
        return False

    def download_album(self, album_info, album_index, total_album):
        """ä¸‹è½½å•ä¸ªç›¸å†Œï¼Œå¸¦è¿›åº¦æ ‡è¯†ï¼Œè·¨å¹³å°å…¼å®¹"""
        sanitized_name, original_name, album_url = album_info
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸŠ [ä¸“è¾‘ {album_index}/{total_album}] å¼€å§‹å¤„ç†: {original_name}")
        logger.info(f"ğŸ“š ç›¸å†Œé“¾æ¥: {album_url}")
        logger.info(f"{'='*60}")

        # åˆ›å»ºç›¸å†Œç›®å½•ï¼ˆè·¨å¹³å°ï¼‰
        album_dir = os.path.join(self.save_path, sanitized_name)
        os.makedirs(album_dir, exist_ok=True)

        try:
            # éšæœºå»¶è¿Ÿ4-8ç§’
            delay = random.uniform(4, 8)
            logger.info(f"â±ï¸  éšæœºå»¶è¿Ÿ {delay:.1f} ç§’...")
            time.sleep(delay)

            # è·å–ç›¸å†Œé¡µé¢å†…å®¹
            response = self.session.get(album_url, timeout=30)
            response.raise_for_status()

            # è§£æç›¸å†Œé¡µé¢ï¼Œè·å–æ‰€æœ‰å›¾ç‰‡é“¾æ¥
            logger.info(f"ğŸ” æå–å›¾ç‰‡é“¾æ¥...")
            soup = BeautifulSoup(response.text, 'html.parser')
            image_urls = []
            for img in soup.find_all('img'):
                if 'src' in img.attrs and img['src'].endswith(('.jpg', '.jpeg', '.png', '.gif')):
                    image_urls.append(img['src'])

            total_images = len(image_urls)
            logger.info(f"ğŸ“¸ ç›¸å†ŒåŒ…å« {total_images} å¼ æœ‰æ•ˆå›¾ç‰‡")
            if total_images == 0:
                logger.error(f"âŒ æ— æœ‰æ•ˆå›¾ç‰‡ï¼ŒåŠ å…¥å¤±è´¥åˆ—è¡¨")
                self.failed_list.append(original_name
