#!/usr/bin/env python3
# -*- coding: utf-8 -*-
""" å‡¸å‡¹å§çˆ¬è™« åŒå¹³å°é€šç”¨ç‰ˆï¼ˆæ‰‹æœºTermux+Windowsç”µè„‘ï¼‰| é¡ºåºä¸‹è½½ | 40KBè¿‡æ»¤ """
import os
import re
import time
import random
import argparse
import logging
import io
import sys
from urllib.parse import urljoin, urlparse
from typing import List, Tuple, Dict, Optional, Set, Any
import requests
from requests.adapters import HTTPAdapter
from requests.exceptions import RequestException
from bs4 import BeautifulSoup

# -------- æ£€æŸ¥ Pillow åº“ --------
try:
    from PIL import Image, ImageFile
    from PIL.Image import UnidentifiedImageError
    ImageFile.LOAD_TRUNCATED_IMAGES = True
    Image.MAX_IMAGE_PIXELS = None
    PILLOW_AVAILABLE = True
except ImportError:
    PILLOW_AVAILABLE = False

# -------- è·¨å¹³å°é»˜è®¤é…ç½® --------
BASE_URL = "https://www.tuao.cc/"
CATEGORIES = [
    ("æœ€æ–°", "/Articles"),
    ("æ— åœ£å…‰", "/Articles/Categories/1"),
    ("å‡¸å‡¹å›¾", "/Articles/Categories/2"),
    ("é“äººä½“", "/Articles/Categories/3"),
    ("å†™çœŸé›†", "/Articles/Categories/4")
]
# è‡ªåŠ¨è¯†åˆ«å¹³å° - æ‰‹æœº(Termux)/Windows
IS_MOBILE = os.path.exists("/sdcard/Download")
# è·¨å¹³å°é»˜è®¤ä¿å­˜è·¯å¾„
DEFAULT_SAVE_DIR_MOBILE = "/sdcard/Download/å‡¹å‡¸å§"
DEFAULT_SAVE_DIR_WIN = r"C:\çˆ¬å–ç»“æœ\å‡¹å‡¸å§"
DEFAULT_SAVE_DIR = DEFAULT_SAVE_DIR_MOBILE if IS_MOBILE else DEFAULT_SAVE_DIR_WIN

DEFAULT_RETRIES = 5
DEFAULT_TIMEOUT = 20
DEFAULT_CONCURRENCY_IMAGE = 4  # ä¸“è¾‘å†…å¹¶å‘ä¸‹è½½å›¾ç‰‡æ•°
DEFAULT_PAGE_SLEEP_MIN = 4.0
DEFAULT_PAGE_SLEEP_MAX = 8.0
DEFAULT_ALBUM_SLEEP_MIN = 4.0
DEFAULT_ALBUM_SLEEP_MAX = 8.0
DEFAULT_POOL_SIZE = 32
MIN_IMAGE_SIZE = 40 * 1024  # 40KBï¼Œè¿‡æ»¤å°äºæ­¤å¤§å°çš„æ–‡ä»¶

# -------- æ—¥å¿—è®¾ç½®ï¼ˆç»ˆç«¯+æ–‡ä»¶ï¼ŒåŒå¹³å°å…¼å®¹ï¼‰ --------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("å‡¹å‡¸å§_crawler.log", encoding="utf-8"),
        logging.StreamHandler(sys.stdout)
    ]
)
if not PILLOW_AVAILABLE:
    logging.warning("Pillow åº“æœªå®‰è£…ï¼Œè·³è¿‡å›¾ç‰‡å®Œæ•´æ€§æ ¡éªŒï¼")
    logging.warning("æ‰‹æœºTermuxå®‰è£…ï¼špkg install python-pillow | Windowså®‰è£…ï¼špip install pillow")

# -------- è¾…åŠ©å‡½æ•° --------
def make_session() -> requests.Session:
    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
        "Referer": BASE_URL,
        "Accept-Language": "zh-CN,zh;q=0.9"
    })
    adapter = HTTPAdapter(
        pool_connections=DEFAULT_POOL_SIZE,
        pool_maxsize=DEFAULT_POOL_SIZE
    )
    s.mount('http://', adapter)
    s.mount('https://', adapter)
    return s

def get_random_delay(min_delay: float, max_delay: float) -> float:
    return random.uniform(min_delay, max_delay)

def request_with_retry(session: requests.Session, url: str, retries: int, timeout: int, is_binary: bool = False) -> Optional[Any]:
    r: Optional[requests.Response] = None
    for attempt in range(1, retries + 1):
        try:
            if is_binary:
                r = session.get(url, timeout=timeout, stream=True)
            else:
                r = session.get(url, timeout=timeout)
            r.raise_for_status()
            return r.content if is_binary else r.text
        except RequestException as e:
            wait_time = get_random_delay(4.0, 8.0)
            status_msg = f"{r.status_code}" if r is not None else "æ— å“åº”"
            if attempt < retries:
                logging.warning("è¯·æ±‚å¤±è´¥: %s (å°è¯• %d/%d) é”™è¯¯: %sã€‚çŠ¶æ€: %sï¼Œç­‰å¾… %.1fs å¹¶é‡è¯•ã€‚", url, attempt, retries, e, status_msg, wait_time)
                time.sleep(wait_time)
            else:
                logging.error("è¯·æ±‚å¤±è´¥: %s (æ‰€æœ‰å°è¯•å‡å¤±è´¥)ã€‚é”™è¯¯: %s", url, e)
                input("æŒ‰ä»»æ„é”®ç»§ç»­...")
    return None

def sanitize_filename(name: str, maxlen: int = 150) -> str:
    if not name:
        return "untitled"
    # è·¨å¹³å°ç‰¹æ®Šå­—ç¬¦è¿‡æ»¤ï¼ˆå…¼å®¹Windows/Androidï¼‰
    s = re.sub(r'[\\/:*?"<>|+@#$%^&*(){}[]]', "_", name).strip()
    return s[:maxlen] or "untitled"

# -------- å›¾åƒéªŒè¯è¾…åŠ©å‡½æ•° --------
def is_image_valid_bytes(data: bytes, verify: bool) -> bool:
    if not verify or not PILLOW_AVAILABLE:
        return True
    try:
        with Image.open(io.BytesIO(data)) as img:
            img.verify()
        return True
    except Exception as e:
        logging.debug("å›¾ç‰‡å†…å®¹æ— æ•ˆ: %s", e)
        return False

def is_image_valid_file(filepath: str, verify: bool) -> bool:
    if not verify or not PILLOW_AVAILABLE:
        return True
    try:
        with Image.open(filepath) as img:
            img.verify()
        return True
    except Exception as e:
        logging.debug("æ–‡ä»¶æŸå: %s", e)
        return False

def save_bytes_atomic(path: str, data: bytes) -> bool:
    tmp_path = path + ".part"
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨ï¼ˆè·¨å¹³å°å…¼å®¹ï¼‰
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(tmp_path, "wb") as f:
            f.write(data)
        os.replace(tmp_path, path)
        return True
    except IOError as e:
        logging.error("å†™å…¥å¤±è´¥ %s: %s", path, e)
        if os.path.exists(tmp_path):
            try:
                os.remove(tmp_path)
            except OSError:
                pass
        return False

# -------- è§£æå‡½æ•° --------
def parse_next_page(soup: BeautifulSoup) -> Optional[str]:
    pagination = soup.find("ul", class_="pagination")
    if not pagination:
        return None
    next_page = pagination.find("a", string="Â»")
    if next_page and "href" in next_page.attrs:
        return next_page["href"]
    return None

def parse_albums_on_listing_page(html: str, base_url: str) -> List[Tuple[str, str]]:
    soup = BeautifulSoup(html, "html.parser")
    albums = []
    img_links = soup.find_all("a", class_="index-imgcontent-img")
    title_links = soup.find_all("a", class_="index-imgcontent-title")
    for i, img_link in enumerate(img_links):
        img_href = img_link.get("href")
        if not img_href:
            continue
        album_url = urljoin(base_url, img_href)
        album_title = ""
        if i < len(title_links):
            album_title = title_links[i].get_text(strip=True)
        if not album_title:
            album_title = f"ä¸“è¾‘_{i+1}"
        album_title = sanitize_filename(album_title)
        albums.append((album_title, album_url))
    return albums

def parse_images_on_album_page(html: str, base_url: str) -> Set[str]:
    soup = BeautifulSoup(html, "html.parser")
    image_urls = set()
    for img in soup.find_all("img"):
        src = img.get("src", "")
        if src.startswith("/Files/images/") and (src.endswith(".webp") or src.endswith(".jpg") or src.endswith(".png")):
            image_urls.add(urljoin(base_url, src))
    logging.info(f"âœ… æœ¬é¡µæŠ“åˆ°å›¾ç‰‡æ•°: {len(image_urls)}")
    return image_urls

# -------- ä¸‹è½½æ ¸å¿ƒé€»è¾‘ --------
def download_single_image(session: requests.Session, url: str, album_dir: str, verify: bool, retries: int, timeout: int, current_index: int, total_images: int) -> str:
    filename = os.path.basename(urlparse(url).path)
    # è¿‡æ»¤æ–‡ä»¶åç‰¹æ®Šå­—ç¬¦ï¼ˆè·¨å¹³å°ï¼‰
    filename = sanitize_filename(filename)
    dest_path = os.path.join(album_dir, filename)
    progress_prefix = f"({current_index}/{total_images})"

    if os.path.exists(dest_path):
        if is_image_valid_file(dest_path, verify):
            logging.info("%s å·²å­˜åœ¨ï¼Œè·³è¿‡: %s", progress_prefix, dest_path)
            return "skipped"
        else:
            logging.warning("%s æ–‡ä»¶æŸåï¼Œé‡æ–°ä¸‹è½½", progress_prefix)
            try:
                os.remove(dest_path)
            except:
                pass

    data = request_with_retry(session, url, retries=retries, timeout=timeout, is_binary=True)
    if not data:
        logging.warning("%s ä¸‹è½½å¤±è´¥: %s", progress_prefix, url)
        return "fail"

    # è¿‡æ»¤å°äº 40KB çš„æ–‡ä»¶
    if len(data) < MIN_IMAGE_SIZE:
        logging.warning("%s æ–‡ä»¶å¤ªå° (%d bytes < 40KB)ï¼Œä¸¢å¼ƒ: %s", progress_prefix, len(data), url)
        return "fail"

    if not is_image_valid_bytes(data, verify):
        logging.warning("%s å›¾ç‰‡æ— æ•ˆï¼Œä¸¢å¼ƒ: %s", progress_prefix, url)
        return "fail"

    if save_bytes_atomic(dest_path, data):
        logging.info("âœ… %s ä¸‹è½½æˆåŠŸ: %s", progress_prefix, dest_path)
        return "ok"
    else:
        logging.warning("âŒ %s ä¿å­˜å¤±è´¥", progress_prefix)
        return "fail"

def parse_album_total_pages(html: str) -> int:
    soup = BeautifulSoup(html, "html.parser")
    pagination = soup.find("ul", class_="pagination")
    if not pagination:
        return 1
    page_numbers = {1}
    for li in pagination.find_all("li"):
        a_tag = li.find("a")
        if a_tag:
            text = a_tag.get_text(strip=True)
            if text.isdigit():
                page_numbers.add(int(text))
    return max(page_numbers) if page_numbers else 1

def process_album(session: requests.Session, title: str, url: str, save_root: str, verify: bool, retries: int, timeout: int) -> Dict[str, int]:
    """å¤„ç†å•ä¸ªä¸“è¾‘ï¼šè¿›å…¥ä¸“è¾‘é¡µ -> æå–å›¾ç‰‡ -> å¹¶å‘ä¸‹è½½å›¾ç‰‡"""
    time.sleep(get_random_delay(DEFAULT_ALBUM_SLEEP_MIN, DEFAULT_ALBUM_SLEEP_MAX))
    log_prefix = f"[ä¸“è¾‘] {title}"
    logging.info("%s â†’ å¼€å§‹å¤„ç†", log_prefix)

    album_html = request_with_retry(session, url, retries=retries, timeout=timeout)
    if not album_html:
        logging.error("%s æ— æ³•è·å–é¡µé¢", log_prefix)
        return {"ok":0,"skipped":0,"fail":1}

    total_pages = parse_album_total_pages(album_html)
    logging.info("%s â†’ å…± %d é¡µ", log_prefix, total_pages)

    all_image_urls = set()
    for page_num in range(1, total_pages+1):
        if page_num == 1:
            page_url = url
        else:
            page_url = f"{url}?page={page_num}"
        logging.info("%s â†’ æŠ“å–åˆ†é¡µ %d/%d", log_prefix, page_num, total_pages)
        page_html = request_with_retry(session, page_url, retries=retries, timeout=timeout)
        if not page_html:
            continue
        imgs = parse_images_on_album_page(page_html, BASE_URL)
        all_image_urls.update(imgs)
        time.sleep(1)

    logging.info("%s â†’ æ€»å…±æŠ“åˆ° %d å¼ å›¾ç‰‡", log_prefix, len(all_image_urls))
    if not all_image_urls:
        return {"ok":0,"skipped":0,"fail":0}

    album_dir = os.path.join(save_root, title)
    os.makedirs(album_dir, exist_ok=True)
    results = {"ok":0,"skipped":0,"fail":0}
    indexed = list(enumerate(all_image_urls, start=1))

    # ä¸“è¾‘å†…å¹¶å‘ä¸‹è½½å›¾ç‰‡
    from concurrent.futures import ThreadPoolExecutor, as_completed
    with ThreadPoolExecutor(max_workers=DEFAULT_CONCURRENCY_IMAGE) as executor:
        future_map = {
            executor.submit(download_single_image, session, img_url, album_dir, verify, retries, timeout, idx, len(all_image_urls)): img_url
            for idx, img_url in indexed
        }
        for f in as_completed(future_map):
            try:
                res = f.result()
                results[res] += 1
            except:
                results["fail"] += 1

    logging.info("%s â†’ å®Œæˆï¼šæˆåŠŸ %d è·³è¿‡ %d å¤±è´¥ %d", log_prefix, results["ok"], results["skipped"], results["fail"])
    return results

# -------- ä¸»å‡½æ•°ï¼šåŒå¹³å°é€šç”¨+é¡ºåºæ‰§è¡Œ --------
def main():
    parser = argparse.ArgumentParser(description="å‡¸å‡¹å§çˆ¬è™«ï¼ˆåŒå¹³å°é€šç”¨ç‰ˆ | æ‰‹æœºTermux+Windowsï¼‰")
    parser.add_argument("--verify", action="store_true", default=True, help="å¼€å¯å›¾ç‰‡å®Œæ•´æ€§æ ¡éªŒï¼ˆé»˜è®¤å¼€å¯ï¼‰")
    parser.add_argument("--no-verify", action="store_false", dest="verify", help="å…³é—­å›¾ç‰‡å®Œæ•´æ€§æ ¡éªŒï¼ŒåŠ å¿«ä¸‹è½½")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨é»˜è®¤è·¯å¾„ï¼Œæ— éœ€æ‰‹åŠ¨è¾“å…¥")
    parser.add_argument("--save-dir", type=str, default="", help="è‡ªå®šä¹‰ä¿å­˜è·¯å¾„ï¼ˆè·¨å¹³å°å…¼å®¹ï¼Œå¦‚/sdcard/Download/xxx æˆ– C:/xxxï¼‰")
    args = parser.parse_args()

    # é…ç½®ä¿å­˜è·¯å¾„
    if args.save_dir:
        save_dir = args.save_dir
    else:
        save_dir = DEFAULT_SAVE_DIR
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(save_dir, exist_ok=True)

    # æ‰“å°å¯åŠ¨ä¿¡æ¯ï¼ˆåŒå¹³å°ï¼‰
    logging.info("="*70)
    logging.info(f"ğŸ“± è¿è¡Œå¹³å°ï¼š{'æ‰‹æœºTermux' if IS_MOBILE else 'Windowsç”µè„‘'}")
    logging.info(f"ğŸ“‚ ä¿å­˜è·¯å¾„ï¼š{save_dir}")
    logging.info(f"ğŸ” å›¾ç‰‡æ ¡éªŒï¼š{'å¼€å¯' if args.verify and PILLOW_AVAILABLE else 'å…³é—­'}")
    logging.info(f"âš¡ è¿‡æ»¤è§„åˆ™ï¼šå°äº40KBçš„æ–‡ä»¶è‡ªåŠ¨ä¸¢å¼ƒ")
    logging.info("="*70)

    session = make_session()
    summary = {"ok":0,"skipped":0,"fail":0,"albums_processed":0}
    seen_album_urls: Set[str] = set()

    # éå†æ‰€æœ‰åˆ†ç±»
    for category_index, (category_name, category_path) in enumerate(CATEGORIES, 1):
        logging.info("\n" + "="*60)
        logging.info(f"[{category_index}/{len(CATEGORIES)}] æ­£åœ¨å¤„ç†åˆ†ç±»: {category_name}")
        logging.info("="*60)

        category_url = urljoin(BASE_URL, category_path)
        current_url = category_url
        page_count = 1

        # éå†å½“å‰åˆ†ç±»çš„æ‰€æœ‰åˆ—è¡¨é¡µ
        while True:
            logging.info(f"[åˆ†ç±»: {category_name}] åˆ—è¡¨é¡µ {page_count}: {current_url}")
            list_html = request_with_retry(session, current_url, retries=DEFAULT_RETRIES, timeout=DEFAULT_TIMEOUT)
            if not list_html:
                logging.warning("è·å–åˆ—è¡¨é¡µå¤±è´¥: %s", current_url)
                time.sleep(get_random_delay(DEFAULT_PAGE_SLEEP_MIN, DEFAULT_PAGE_SLEEP_MAX))
                break

            # è§£æå½“å‰é¡µçš„æ‰€æœ‰ä¸“è¾‘
            current_page_albums = parse_albums_on_listing_page(list_html, BASE_URL)
            logging.info(f"[åˆ†ç±»: {category_name}] åˆ—è¡¨é¡µ {page_count} å‘ç° {len(current_page_albums)} ä¸ªä¸“è¾‘")

            # é¡ºåºå¤„ç†å½“å‰é¡µçš„æ¯ä¸ªä¸“è¾‘ï¼šè·å–ä¸€ä¸ªï¼Œä¸‹è½½ä¸€ä¸ª
            for album_title, album_url in current_page_albums:
                if album_url in seen_album_urls:
                    logging.info(f"è·³è¿‡å·²å¤„ç†ä¸“è¾‘: {album_title}")
                    continue
                seen_album_urls.add(album_url)

                # ç«‹å³ä¸‹è½½è¿™ä¸ªä¸“è¾‘
                result = process_album(
                    session,
                    album_title,
                    album_url,
                    save_dir,
                    args.verify,
                    DEFAULT_RETRIES,
                    DEFAULT_TIMEOUT
                )

                # æ›´æ–°ç»Ÿè®¡
                summary["ok"] += result["ok"]
                summary["skipped"] += result["skipped"]
                summary["fail"] += result["fail"]
                summary["albums_processed"] += 1

            # æ£€æŸ¥ä¸‹ä¸€é¡µ
            soup = BeautifulSoup(list_html, "html.parser")
            next_page_url = parse_next_page(soup)
            if next_page_url:
                current_url = urljoin(BASE_URL, next_page_url)
                page_count += 1
                logging.info(f"[{category_name}] å‘ç°ä¸‹ä¸€é¡µï¼Œå°†ç»§ç»­çˆ¬å–ç¬¬ {page_count} é¡µ")
                time.sleep(get_random_delay(DEFAULT_PAGE_SLEEP_MIN, DEFAULT_PAGE_SLEEP_MAX))
            else:
                logging.info(f"[{category_index}/{len(CATEGORIES)}] ç±»å‹ {category_name} å·²å®Œæˆæ‰€æœ‰åˆ†é¡µçˆ¬å–")
                break

    # æ‰“å°æœ€ç»ˆç»“æœ
    logging.info("\n" + "="*70)
    logging.info("ç¨‹åºæ‰§è¡Œå®Œæ¯•ã€‚çˆ¬å–ä»»åŠ¡æ€»ç»“ï¼š")
    logging.info(f" å¤„ç†çš„ä¸“è¾‘æ€»æ•°: {summary['albums_processed']}")
    logging.info("-" * 25)
    logging.info(f" [æˆåŠŸä¸‹è½½]: {summary['ok']} å¼ ")
    logging.info(f" [è·³è¿‡ (å·²å­˜åœ¨)]: {summary['skipped']} å¼ ")
    logging.info(f" [å¤±è´¥æ€»æ•°]: {summary['fail']} å¼ ")
    logging.info("="*70)
    logging.info(f"æ‰€æœ‰å›¾ç‰‡å·²ä¿å­˜åˆ°: {save_dir}")
    if IS_MOBILE:
        logging.info("ğŸ“± æ‰‹æœºæ–‡ä»¶ç®¡ç†å™¨æŸ¥æ‰¾ï¼šå†…éƒ¨å­˜å‚¨ â†’ Download â†’ å‡¹å‡¸å§")
    else:
        logging.info("ğŸ’» WindowsæŸ¥æ‰¾ï¼šæ­¤ç”µè„‘ â†’ Cç›˜ â†’ çˆ¬å–ç»“æœ â†’ å‡¹å‡¸å§")

if __name__ == "__main__":
    main()
