# -*- coding: utf-8 -*-
""" ç¾å›¾è‰²è‰²çˆ¬è™« åŒå¹³å°é€šç”¨ç‰ˆï¼ˆæ‰‹æœºTermux+Windowsç”µè„‘ï¼‰| é¡ºåºä¸‹è½½ | 40KBè¿‡æ»¤ """
import os
import re
import time
import random
import sys
import requests
import argparse
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from bs4 import BeautifulSoup
from PIL import Image
from io import BytesIO
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# -------- æ—¥å¿—è®¾ç½®ï¼ˆåŒå¹³å°å…¼å®¹ï¼šç»ˆç«¯+æ–‡ä»¶ï¼‰ --------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("ç¾å›¾è‰²è‰²_crawler.log", encoding="utf-8"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# -------- è·¨å¹³å°æ ¸å¿ƒé…ç½® --------
IS_MOBILE = os.path.exists("/sdcard/Download")  # è‡ªåŠ¨è¯†åˆ«æ‰‹æœº/Windows
# è·¨å¹³å°é»˜è®¤ä¿å­˜è·¯å¾„
DEFAULT_SAVE_DIR_MOBILE = "/sdcard/Download/ç¾å›¾è‰²è‰²"
DEFAULT_SAVE_DIR_WIN = r"C:\çˆ¬å–ç»“æœ\ç¾å›¾è‰²è‰²"
MIN_IMAGE_SIZE = 40 * 1024  # 40KBè¿‡æ»¤

class MeituSpider:
    def __init__(self, save_path, verify=False, page_sleep=5, album_sleep=3):
        self.save_path = save_path
        self.verify = verify
        self.page_sleep = page_sleep
        self.album_sleep = album_sleep
        self.session = self._init_session()
        self.base_url = "https://xn--drdgbhrb-xx6n10qjm3s.tljkd-01.sbs"
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36"
        ]
        self.failed_images = []
        self.failed_albums = []
        self.processed_album_urls = set()  # è®°å½•å·²å¤„ç†ä¸“è¾‘ï¼Œé¿å…é‡å¤

    def _init_session(self):
        session = requests.Session()
        retry = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504]
        )
        adapter = HTTPAdapter(max_retries=retry, pool_connections=100, pool_maxsize=100)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        return session

    def _get_random_user_agent(self):
        return random.choice(self.user_agents)

    def _get_response(self, url, retries=5):
        headers = {
            "User-Agent": self._get_random_user_agent(),
            "Referer": self.base_url,
            "Accept-Language": "zh-CN,zh;q=0.9"
        }
        
        for i in range(retries):
            start_time = time.time()
            try:
                response = self.session.get(url, headers=headers, timeout=30)
                response.raise_for_status()
                request_time = time.time() - start_time
                logger.info(f"[è¯·æ±‚] {url} æˆåŠŸï¼Œè€—æ—¶ {request_time:.2f}ç§’")
                
                delay = random.uniform(2, 4)
                time.sleep(delay)
                return response
            except Exception as e:
                request_time = time.time() - start_time
                if i < retries - 1:
                    delay = random.uniform(4, 8)
                    logger.warning(f"[è¯·æ±‚] {url} å¤±è´¥ï¼Œ{i+1}/{retries} é‡è¯•ï¼Œ{delay:.2f}ç§’åé‡è¯•: {e}")
                    time.sleep(delay)
                else:
                    logger.error(f"[è¯·æ±‚] {url} å¤±è´¥ï¼Œ{retries} æ¬¡é‡è¯•åä»å¤±è´¥: {e}")
                    return None

    def _parse_albums(self, url):
        """è§£æå•é¡µç›¸å†Œåˆ—è¡¨ï¼Œè¿”å›æœ¬é¡µæ‰€æœ‰ä¸“è¾‘"""
        response = self._get_response(url)
        if not response:
            logger.error(f"[è§£æ] è¯·æ±‚å¤±è´¥ï¼Œæ— æ³•è§£æé¡µé¢: {url}")
            return [], None
        
        try:
            soup = BeautifulSoup(response.text, "html.parser")
            albums = []
            album_elements = soup.select(".videos-list-wrap .video-item-col")
            logger.info(f"[è§£æ] æœ¬é¡µæ‰¾åˆ° {len(album_elements)} ä¸ªç›¸å†Œå…ƒç´ ")
            
            for i, album in enumerate(album_elements):
                try:
                    album_url = album.get("href")
                    album_title = album.select_one(".video-desc-content").text.strip() if album.select_one(".video-desc-content") else f"æœªçŸ¥ä¸“è¾‘_{i+1}"
                    # è·¨å¹³å°è¿‡æ»¤ä¸“è¾‘åç‰¹æ®Šå­—ç¬¦
                    album_title = re.sub(r'[\\/:*?"<>|+@#$%^&*(){}[]]', "_", album_title)
                    if album_url and album_title not in ["/", ""]:
                        full_url = f"{self.base_url}{album_url}" if not album_url.startswith("http") else album_url
                        albums.append((album_title, full_url))
                        logger.debug(f"[è§£æ] è§£æåˆ°ç›¸å†Œ: {album_title} - {full_url}")
                except Exception as e:
                    logger.error(f"[è§£æ] è§£æç›¸å†Œå…ƒç´ å¤±è´¥: {e}")
            
            # è§£æä¸‹ä¸€é¡µ
            next_page = None
            next_page_element = soup.select_one(".mo-paging .paging-item--next")
            if next_page_element and next_page_element.get("href"):
                next_page = f"{self.base_url}{next_page_element.get('href')}" if not next_page_element.get("href").startswith("http") else next_page_element.get("href")
            return albums, next_page
        except Exception as e:
            logger.error(f"[è§£æ] è§£æé¡µé¢å¤±è´¥: {url}, é”™è¯¯: {e}")
            return [], None

    def _parse_album_images(self, album_url):
        logger.info(f"[è§£æ] å¼€å§‹è§£æç›¸å†Œå›¾ç‰‡: {album_url}")
        response = self._get_response(album_url)
        if not response:
            return []
        
        soup = BeautifulSoup(response.text, "html.parser")
        book_pages = soup.select_one("#book-pages")
        if not book_pages:
            logger.error(f"[è§£æ] æœªæ‰¾åˆ°ç›¸å†Œå›¾ç‰‡å®¹å™¨: {album_url}")
            return []
        
        screenshots = book_pages.get("data-screenshots", "")
        if not screenshots:
            logger.error(f"[è§£æ] æœªæ‰¾åˆ°ç›¸å†Œå›¾ç‰‡URL: {album_url}")
            return []
        
        images = []
        for img_url in screenshots.split("#$"):
            img_url = img_url.strip().lstrip('$')
            if img_url and img_url.startswith("http"):
                images.append(img_url)
        logger.info(f"[è§£æ] ä»ç›¸å†Œæå–åˆ° {len(images)} å¼ å›¾ç‰‡")
        return images

    def _validate_image(self, image_path):
        try:
            with Image.open(image_path) as img:
                img.verify()
            return True
        except Exception:
            return False

    def _download_image(self, img_url, save_path):
        if os.path.exists(save_path):
            logger.info(f"[ä¸‹è½½] å›¾ç‰‡å·²å­˜åœ¨ï¼Œè·³è¿‡: {save_path}")
            return True
        
        logger.info(f"[ä¸‹è½½] å¼€å§‹ä¸‹è½½: {img_url}")
        response = self._get_response(img_url)
        if response is None:
            self.failed_images.append((img_url, save_path))
            return False
        
        # è¿‡æ»¤éå›¾ç‰‡/è¿‡å°æ–‡ä»¶
        content_type = response.headers.get("Content-Type", "")
        file_size = len(response.content)
        if not content_type.startswith("image/"):
            logger.error(f"[ä¸‹è½½] éå›¾ç‰‡å†…å®¹ï¼Œä¸¢å¼ƒ: {img_url}")
            self.failed_images.append((img_url, save_path))
            return False
        if file_size < MIN_IMAGE_SIZE:  # 40KBè¿‡æ»¤
            logger.error(f"[ä¸‹è½½] æ–‡ä»¶è¿‡å°({file_size}å­—èŠ‚)ï¼Œä¸¢å¼ƒ: {img_url}")
            self.failed_images.append((img_url, save_path))
            return False
        
        # åŸå­åŒ–å†™å…¥ï¼ˆè·¨å¹³å°ç›®å½•å…¼å®¹ï¼‰
        save_dir = os.path.dirname(save_path)
        os.makedirs(save_dir, exist_ok=True)
        temp_path = f"{save_path}.tmp"
        try:
            with open(temp_path, "wb") as f:
                f.write(response.content)
        except Exception as e:
            logger.error(f"[ä¸‹è½½] å†™å…¥å¤±è´¥: {e}")
            self.failed_images.append((img_url, save_path))
            return False
        
        # å›¾ç‰‡éªŒè¯
        if self.verify:
            if not self._validate_image(temp_path):
                logger.error(f"[ä¸‹è½½] å›¾ç‰‡æŸåï¼Œä¸¢å¼ƒ: {img_url}")
                os.remove(temp_path)
                self.failed_images.append((img_url, save_path))
                return False
        
        # é‡å‘½åå®Œæˆä¸‹è½½
        try:
            os.rename(temp_path, save_path)
            logger.info(f"[ä¸‹è½½] æˆåŠŸä¿å­˜: {save_path}")
            return True
        except Exception as e:
            logger.error(f"[ä¸‹è½½] é‡å‘½åå¤±è´¥: {e}")
            os.remove(temp_path)
            self.failed_images.append((img_url, save_path))
            return False

    def _download_album(self, album_info, album_index, total_album):
        """ä¸‹è½½å•ä¸ªä¸“è¾‘"""
        album_title, album_url = album_info
        if album_url in self.processed_album_urls:
            logger.info(f"[ä¸“è¾‘ {album_index}/{total_album}] å·²å¤„ç†ï¼Œè·³è¿‡: {album_title}")
            return True
        self.processed_album_urls.add(album_url)
        
        logger.info(f"\n[ä¸“è¾‘ {album_index}/{total_album}] å¼€å§‹å¤„ç†: {album_title}")
        # æ¸…æ´—ä¸“è¾‘åï¼Œè·¨å¹³å°è·¯å¾„å…¼å®¹
        safe_title = re.sub(r'[<>"/\\|?*:]', "_", album_title)[:50] if album_title else f"ä¸“è¾‘_{album_index}"
        album_dir = os.path.join(self.save_path, safe_title)
        os.makedirs(album_dir, exist_ok=True)
        
        # è§£æå›¾ç‰‡å¹¶ä¸‹è½½
        images = self._parse_album_images(album_url)
        if not images:
            logger.error(f"[ä¸“è¾‘ {album_index}/{total_album}] æ— å›¾ç‰‡ï¼ŒåŠ å…¥å¤±è´¥åˆ—è¡¨")
            self.failed_albums.append(album_info)
            return False
        
        # å•ä¸“è¾‘å†…å¹¶å‘ä¸‹è½½å›¾ç‰‡ï¼ˆä½å¹¶å‘é˜²åçˆ¬ï¼‰
        image_failures = 0
        with ThreadPoolExecutor(max_workers=8) as executor:
            futures = []
            for img_index, img_url in enumerate(images):
                img_name = f"{img_index+1:03d}.jpg"
                img_path = os.path.join(album_dir, img_name)
                futures.append(executor.submit(self._download_image, img_url, img_path))
            
            for future in as_completed(futures):
                if not future.result():
                    image_failures += 1
        
        logger.info(f"[ä¸“è¾‘ {album_index}/{total_album}] å¤„ç†å®Œæˆ: æˆåŠŸ{len(images)-image_failures}å¼  | å¤±è´¥{image_failures}å¼ ")
        time.sleep(self.album_sleep)  # ä¸“è¾‘é—´å»¶è¿Ÿï¼Œåçˆ¬
        return True

    def _retry_failed(self):
        """é‡è¯•å¤±è´¥çš„å›¾ç‰‡å’Œä¸“è¾‘"""
        total_failed = len(self.failed_albums) + len(self.failed_images)
        if total_failed == 0:
            return
        logger.info(f"\n[é‡è¯•] å¼€å§‹å¤„ç†å¤±è´¥é¡¹: ä¸“è¾‘{len(self.failed_albums)}ä¸ª | å›¾ç‰‡{len(self.failed_images)}å¼ ")
        # é‡è¯•ä¸“è¾‘
        for idx, album in enumerate(self.failed_albums, 1):
            self._download_album(album, idx, len(self.failed_albums))
        # é‡è¯•å›¾ç‰‡
        success = 0
        for img_url, save_path in self.failed_images:
            if self._download_image(img_url, save_path):
                success += 1
        logger.info(f"[é‡è¯•] å®Œæˆ: å›¾ç‰‡æˆåŠŸ{success}/{len(self.failed_images)}å¼ ")

    def run(self):
        """ä¸»è¿è¡Œé€»è¾‘ï¼šé¡ºåºçˆ¬å–+ä¸‹è½½"""
        start_total_time = time.time()
        logger.info("="*50)
        logger.info(f"ğŸ“± è¿è¡Œå¹³å°ï¼š{'æ‰‹æœºTermux' if IS_MOBILE else 'Windowsç”µè„‘'}")
        logger.info("[ä¸»ç¨‹åº] ç¾å›¾è‰²è‰²çˆ¬è™« - åŒå¹³å°é€šç”¨ç‰ˆ")
        logger.info(f"[ä¸»ç¨‹åº] ä¿å­˜è·¯å¾„: {self.save_path}")
        logger.info(f"[ä¸»ç¨‹åº] å›¾ç‰‡éªŒè¯: {self.verify} | åˆ—è¡¨é¡µå»¶è¿Ÿ: {self.page_sleep}s")
        logger.info(f"[ä¸»ç¨‹åº] è¿‡æ»¤è§„åˆ™: å°äº40KBæ–‡ä»¶è‡ªåŠ¨ä¸¢å¼ƒ")
        logger.info("="*50)
        os.makedirs(self.save_path, exist_ok=True)

        # åˆå§‹åŒ–çˆ¬å–å‚æ•°
        current_url = "https://xn--drdgbhrb-xx6n10qjm3s.tljkd-01.sbs/t/13/"
        page = 1
        processed_pages = set()
        total_album_count = 0  # ç´¯è®¡æ€»ä¸“è¾‘æ•°

        # æ ¸å¿ƒï¼šå¾ªç¯çˆ¬å–åˆ—è¡¨é¡µ â†’ é€ä¸ªå¤„ç†æœ¬é¡µä¸“è¾‘ â†’ çˆ¬å–ä¸‹ä¸€é¡µ
        while current_url and current_url not in processed_pages:
            processed_pages.add(current_url)
            logger.info(f"\n[åˆ—è¡¨é¡µ] å¼€å§‹çˆ¬å–ç¬¬ {page} é¡µ: {current_url}")
            # è§£ææœ¬é¡µæ‰€æœ‰ä¸“è¾‘
            page_albums, next_page = self._parse_albums(current_url)
            if not page_albums:
                logger.warning(f"[åˆ—è¡¨é¡µ] ç¬¬{page}é¡µæ— ä¸“è¾‘ï¼Œè·³è‡³ä¸‹ä¸€é¡µ")
                current_url = next_page
                page += 1
                time.sleep(self.page_sleep)
                continue
            
            # é¡ºåºå¤„ç†æœ¬é¡µæ¯ä¸ªä¸“è¾‘ï¼šè·å–ä¸€ä¸ªï¼Œä¸‹è½½ä¸€ä¸ª
            total_album_count += len(page_albums)
            for idx, album in enumerate(page_albums, 1):
                self._download_album(album, total_album_count - len(page_albums) + idx, total_album_count)
            
            # è·³è‡³ä¸‹ä¸€é¡µ
            if next_page and next_page not in processed_pages:
                logger.info(f"[åˆ—è¡¨é¡µ] ç¬¬{page}é¡µå¤„ç†å®Œæˆï¼Œè·³è‡³ä¸‹ä¸€é¡µ")
                current_url = next_page
                page += 1
                time.sleep(self.page_sleep)
            else:
                logger.info(f"[åˆ—è¡¨é¡µ] å·²çˆ¬å–æ‰€æœ‰é¡µé¢ï¼ˆå…±{page}é¡µï¼‰")
                current_url = None

        # é‡è¯•å¤±è´¥é¡¹
        self._retry_failed()

        # æœ€ç»ˆç»Ÿè®¡
        total_time = time.time() - start_total_time
        logger.info("\n" + "="*50)
        logger.info("[ä¸»ç¨‹åº] çˆ¬å–å®Œæˆï¼")
        logger.info(f"[ä¸»ç¨‹åº] æ€»è€—æ—¶: {total_time/60:.1f}åˆ†é’Ÿ | å¤„ç†ä¸“è¾‘: {total_album_count}ä¸ª")
        logger.info(f"[ä¸»ç¨‹åº] å¤±è´¥é¡¹: ä¸“è¾‘{len(self.failed_albums)}ä¸ª | å›¾ç‰‡{len(self.failed_images)}å¼ ")
        logger.info(f"[ä¸»ç¨‹åº] å›¾ç‰‡ä¿å­˜è·¯å¾„: {self.save_path}")
        if IS_MOBILE:
            logger.info("ğŸ“± æ‰‹æœºæŸ¥æ‰¾ï¼šå†…éƒ¨å­˜å‚¨ â†’ Download â†’ ç¾å›¾è‰²è‰²")
        else:
            logger.info("ğŸ’» WindowsæŸ¥æ‰¾ï¼šæ­¤ç”µè„‘ â†’ Cç›˜ â†’ çˆ¬å–ç»“æœ â†’ ç¾å›¾è‰²è‰²")
        logger.info("="*50)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ç¾å›¾è‰²è‰²çˆ¬è™«-åŒå¹³å°é€šç”¨ç‰ˆï¼ˆæ‰‹æœºTermux+Windowsï¼‰")
    parser.add_argument("--page-sleep", type=float, default=5, help="åˆ—è¡¨é¡µç¿»é¡µå»¶è¿Ÿ(ç§’)ï¼Œé»˜è®¤5")
    parser.add_argument("--album-sleep", type=float, default=3, help="ä¸“è¾‘é—´ä¸‹è½½å»¶è¿Ÿ(ç§’)ï¼Œé»˜è®¤3")
    parser.add_argument("--no-verify", action="store_true", help="å…³é—­å›¾ç‰‡éªŒè¯ï¼ŒåŠ å¿«ä¸‹è½½é€Ÿåº¦")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨é»˜è®¤è·¯å¾„ï¼Œæ— éœ€è¾“å…¥")
    parser.add_argument("--save-dir", type=str, default="", help="è‡ªå®šä¹‰ä¿å­˜è·¯å¾„ï¼ˆå¦‚/sdcard/Download/xxx æˆ– C:/xxxï¼‰")
    args = parser.parse_args()

    # é…ç½®ä¿å­˜è·¯å¾„
    if args.save_dir:
        save_path = args.save_dir
    else:
        save_path = DEFAULT_SAVE_DIR_MOBILE if IS_MOBILE else DEFAULT_SAVE_DIR_WIN
    # å›¾ç‰‡éªŒè¯å¼€å…³
    verify = not args.no_verify

    # å¯åŠ¨çˆ¬è™«
    spider = MeituSpider(
        save_path=save_path,
        verify=verify,
        page_sleep=args.page_sleep,
        album_sleep=args.album_sleep
    )
    spider.run()
